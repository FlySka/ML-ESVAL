{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librerias y funciones listas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (16, 9)\n",
    "#plt.style.use('ggplot')\n",
    "from subprocess import check_call\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from scipy.integrate import simps\n",
    "from datetime import datetime\n",
    "import sklearn.metrics as metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import initializers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "def norm_minimax(datos_train, datos_test=None, splits=True, mini=None, maxi=None):\n",
    "    if (mini is None) and (maxi is None):\n",
    "        maxi=(np.max(datos_train))\n",
    "        mini=(np.min(datos_train))\n",
    "        datos_train = (datos_train-mini)/(maxi-mini)\n",
    "        return datos_train, mini, maxi\n",
    "    if (mini is not None) & (maxi is not None) & (splits==True):\n",
    "        datos_train = (datos_train-mini)/(maxi-mini)\n",
    "        datos_test = (datos_test-mini)/(maxi-mini)\n",
    "        return datos_train, datos_test\n",
    "    if (mini is not None) & (maxi is not None) & (splits==False):\n",
    "        datos_train = (datos_train-mini)/(maxi-mini)\n",
    "        return datos_train\n",
    "    print('Error de entradas en nomalizacion minimax')\n",
    "    return -1\n",
    "\n",
    "# calcular peso de clase para darselo al modelo\n",
    "def pesos_clases(y_train, only_pos=False):\n",
    "    cant_refa = 0\n",
    "    cant_norefa = 0\n",
    "    for i in range(0, len(y_train)):\n",
    "        if y_train[i] == 1:\n",
    "            cant_refa = cant_refa + 1\n",
    "        else:\n",
    "            cant_norefa = cant_norefa + 1\n",
    "    total = cant_norefa + cant_refa\n",
    "    porcentaje_refa = (cant_refa/total)*100\n",
    "    print('Porcentaje de refacturas: %0.2f' % porcentaje_refa)\n",
    "    weight_for_0 = (1 / cant_norefa)*(total)/2.0 \n",
    "    weight_for_1 = ((1 / cant_refa)*(total)/2.0) #/2 #dividido 2 para que no haya tantos falsos positivos\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    initial_bias = np.log([cant_refa/cant_norefa])\n",
    "    #weight = cant_norefa/cant_refa\n",
    "    print('Peso de clase \"0\" (sin refactura): {:.2f}'.format(weight_for_0))\n",
    "    print('Peso de clase \"1\" (con refactura): {:.2f}'.format(weight_for_1))\n",
    "    print('Bias inicial: ',initial_bias)\n",
    "    if only_pos==False:\n",
    "        return weight_for_0, weight_for_1, initial_bias\n",
    "    else:\n",
    "        return weight_for_1\n",
    "\n",
    "def curva_ROC(fpr, tpr, roc_auc):  \n",
    "    # method I: plt\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc, color='DarkSeaGreen')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'--', color='RoyalBlue')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"ROC\"  + \".jpg\")\n",
    "    plt.show()\n",
    "\n",
    "#definimos funciona para mostrar los resultados\n",
    "def mostrar_resultados(y_test, pred_y, umbral=None):\n",
    "    for i in range(0, len(pred_y)):\n",
    "        if pred_y[i] > umbral:\n",
    "            pred_y[i] = 1\n",
    "        else:\n",
    "            pred_y[i] = 0\n",
    "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", cmap='Blues');\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    #plt.savefig(\"matriz_confusion\"  + \".jpg\")\n",
    "    plt.show()\n",
    "    #print (classification_report(y_test, pred_y))\n",
    "    \n",
    "def MOSTRAR_METRICAS(y_test, pred_y, umbral=None, beta=None):\n",
    "    for i in range(0, len(pred_y)):\n",
    "        if pred_y[i] > umbral:\n",
    "            pred_y[i] = 1\n",
    "        else:\n",
    "            pred_y[i] = 0\n",
    "    [TN, FP],[FN,TP] = confusion_matrix(y_test, pred_y)\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    f1_score = (2*recall*precision) / (recall + precision)\n",
    "    g_mean = np.sqrt((TP / (TP + FN)) * (TN / (TN + FP)))\n",
    "    print('Recall: %0.4f' % recall)\n",
    "    print('Precision: %0.4f' % precision)\n",
    "    print('F1-score: %0.4f' % f1_score)\n",
    "    print('G-mean: %0.4f' % g_mean)\n",
    "    if beta is not None:\n",
    "        f_beta = (((1 + beta)**2) * recall * precision) / (((beta**2) * precision) + recall)\n",
    "        print('F-measure: %0.4f' % f_beta)\n",
    "        return recall, precision, f1_score, g_mean, f_beta\n",
    "    return recall, precision, f1_score, g_mean\n",
    "        \n",
    "def AUCM_PLOT(archivo):\n",
    "    # Calculo de porcentajes\n",
    "    porc_TD = numpy.arange(0, 1.01, 0.01)\n",
    "    porc_TR = numpy.arange(0, 1.01, 0.01)\n",
    "    j = 0\n",
    "    maximo = 0\n",
    "    limite_pasado = False\n",
    "    for umbral in numpy.arange(0, 1.01, 0.01):\n",
    "        predprob_y = joblib.load(archivo)\n",
    "        for i in range(0, len(predprob_y)):\n",
    "            if predprob_y[i] > umbral:\n",
    "                predprob_y[i] = 1\n",
    "            else:\n",
    "                predprob_y[i] = 0\n",
    "        [TN, FP],[FN,TP] = confusion_matrix(y_test, predprob_y)\n",
    "        porc_TD[j] = (TP + FP) / (TP + FP + TN + FN)\n",
    "        porc_TR[j] = TP / (TP + FN)\n",
    "        if maximo < (porc_TR[j] - porc_TD[j]):\n",
    "            maximo = umbral\n",
    "            maximo_registros = TP + FP\n",
    "        if ((TP+FP) < 3500) & (limite_pasado==False):\n",
    "            limite_umbral = umbral\n",
    "            limite_refactura = porc_TR[j]\n",
    "            revisar = TP + FP\n",
    "            limite_pasado = True\n",
    "        j+=1\n",
    "        #print('interacion de: ', umbral)\n",
    "    umbral = numpy.arange(0, 1.01, 0.01)\n",
    "    # Calculo de areas\n",
    "    rocm_total = simps(porc_TD, umbral)\n",
    "    rocm_refa = simps(porc_TR, umbral)\n",
    "    AUCM = rocm_refa - rocm_total\n",
    "    print('AUCM = %0.5f' % AUCM)\n",
    "    #grafica\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title('Receiver Operating Characteristic Modified (ROCM)')\n",
    "    plt.plot(umbral, porc_TD, 'b', label = 'AUC Total = %0.4f' % rocm_total, color='indianred')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot(umbral, porc_TR, 'b', label = 'AUC Refacturas = %0.4f' % rocm_refa, color='DarkSeaGreen')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([1.01, -0.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('% del Total')\n",
    "    plt.xlabel('% probabilidad de refactura desde')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"ROCM\" + \".jpg\")\n",
    "    plt.show()\n",
    "    return limite_umbral, limite_refactura, revisar, maximo, maximo_registros, AUCM\n",
    "\n",
    "def Carga_Datos(archivo=None, limpia=False, filtrado=False):\n",
    "    if limpia==True:\n",
    "        if len(archivo)>1:\n",
    "            dtype = {'NRO_SUMINISTRO': 'str', 'COD_DIAMETRO': 'str', 'LECTURA_TERRENO': 'float32', 'CLAVE_TERRENO': 'str',\n",
    "                     'LECTURA_ANT': 'float32', 'CONSUMO_BASE': 'float32', 'CONSUMO_PROM': 'float32', 'CATEGORIA': 'str',\n",
    "                     'TIP_DOCUMENTO': 'str', 'COD_LECTOR': 'str', 'COD_OBS': 'str', 'COD_LOCALIDAD': 'str', 'RECORR1': 'str',\n",
    "                     'RECORR2': 'str', 'TIE_REMAR': 'str', 'ID_RELACION': 'str', 'COD_EMPRESA': 'str', \n",
    "                     'CLAVE_TERRENO_MES_ANT': 'str', 'CONS_BASE_MES_ANT': 'float32', 'COD_OBS_MES_ANT': 'str',\n",
    "                     'CLAVE_TERRENO_MISMO_MES_ANNO_ANT': 'str', 'CONS_BASE_MISMO_MES_ANNO_ANT': 'float32',\n",
    "                     'COD_OBS_MISMO_MES_ANNO_ANT': 'str', 'ANNO': 'str', 'MES': 'str', 'ANALISTA': 'float32', \n",
    "                     'TIENE_REFA': 'float32'}\n",
    "            data1 = pd.read_csv(archivo[0], delimiter=';', dtype=dtype, low_memory=False)\n",
    "            data2 = pd.read_csv(archivo[1], delimiter=';', dtype=dtype, low_memory=False)\n",
    "            Data = pd.concat([data1, data2], ignore_index=True)\n",
    "            del data1\n",
    "            del data2\n",
    "            Data = Data.drop('Unnamed: 0', axis=1)\n",
    "        else:\n",
    "            dtype = {'NRO_SUMINISTRO': 'str', 'COD_DIAMETRO': 'str', 'LECTURA': 'float32', 'LECTURA_TERRENO': 'float32',\n",
    "                     'CLAVE_TERRENO': 'str', 'LECTURA_ANT': 'float32', 'CONSUMO_BASE': 'float32', 'CONSUMO_PROM': 'float32',\n",
    "                     'CATEGORIA': 'str', 'TIP_DOCUMENTO': 'str', 'COD_LECTOR': 'str', 'COD_OBS': 'str', 'COD_LOCALIDAD': 'str',\n",
    "                     'RECORR1': 'str', 'RECORR2': 'str', 'TIE_REMAR': 'str', 'ID_RELACION': 'str', 'COD_EMPRESA': 'str',\n",
    "                     'CLAVE_LECTURA': 'str', 'CLAVE_TERRENO_MES_ANT': 'str', 'CONS_BASE_MES_ANT': 'float32', 'COD_OBS_MES_ANT': 'str',\n",
    "                     'CLAVE_TERRENO_MISMO_MES_ANNO_ANT': 'str', 'CONS_BASE_MISMO_MES_ANNO_ANT': 'float32',\n",
    "                     'COD_OBS_MISMO_MES_ANNO_ANT': 'str', 'ANNO': 'str', 'MES': 'str', 'ANALISTA': 'float32', 'TIENE_REFA': 'float32'}\n",
    "            Data = pd.read_csv(archivo[0], delimiter=';', dtype=dtype, low_memory=False)\n",
    "            #Data = Data.drop('Unnamed: 0', axis=1)\n",
    "        print('Se Cargo base de datos')\n",
    "        return Data\n",
    "    if len(archivo)>1:\n",
    "        #Se agrega el tipo de variable de cada colunma para visualizaci√≥n\n",
    "        dtype = {'NRO_SUMINISTRO': 'str', 'COD_DIAMETRO': 'str', 'LECTURA': 'float32', 'LECTURA_TERRENO': 'float32',\n",
    "                 'CLAVE_TERRENO': 'str', 'LECTURA_ANT': 'float32', 'CONSUMO_BASE': 'float32', 'CONSUMO_PROM': 'float32',\n",
    "                 'CATEGORIA': 'str', 'TIP_DOCUMENTO': 'str', 'COD_LECTOR': 'str', 'COD_OBS': 'str', 'COD_LOCALIDAD': 'str',\n",
    "                 'RECORR1': 'str', 'RECORR2': 'str', 'TIE_REMAR': 'str', 'ID_RELACION': 'str', 'COD_EMPRESA': 'str',\n",
    "                 'CLAVE_LECTURA': 'str', 'CLAVE_TERRENO_MES_ANT': 'str', 'CONS_BASE_MES_ANT': 'float32', 'COD_OBS_MES_ANT': 'str',\n",
    "                 'CLAVE_TERRENO_MISMO_MES_ANNO_ANT': 'str', 'CONS_BASE_MISMO_MES_ANNO_ANT': 'float32', \n",
    "                 'COD_OBS_MISMO_MES_ANNO_ANT': 'str', 'ANNO': 'str', 'MES': 'str', 'TIENE_REFA': 'float32'}\n",
    "        #se leen los datos\n",
    "        data1 = pd.read_csv(archivo[0], delimiter=';', dtype=dtype, low_memory=False)\n",
    "        data2 = pd.read_csv(archivo[1], delimiter=';', dtype=dtype, low_memory=False)\n",
    "        #se encadenan todos los chunks en una tabla\n",
    "        Data = pd.concat([data1, data2], ignore_index=True)\n",
    "        del data1\n",
    "        del data2\n",
    "        data3 = pd.read_csv(archivo[2], delimiter=';', dtype=dtype, low_memory=False)\n",
    "        Data = pd.concat([Data, data3], ignore_index=True)\n",
    "        del data3\n",
    "        Data = Data.drop([len(Data)-1],axis=0)\n",
    "    else:\n",
    "        dtype = {'NRO_SUMINISTRO': 'str', 'COD_DIAMETRO': 'str', 'LECTURA': 'float32', 'LECTURA_TERRENO': 'float32',\n",
    "                 'CLAVE_TERRENO': 'str', 'LECTURA_ANT': 'float32', 'CONSUMO_BASE': 'float32', 'CONSUMO_PROM': 'float32',\n",
    "                 'CATEGORIA': 'str', 'TIP_DOCUMENTO': 'str', 'COD_LECTOR': 'str', 'COD_OBS': 'str', 'COD_LOCALIDAD': 'str',\n",
    "                 'RECORR1': 'str', 'RECORR2': 'str', 'TIE_REMAR': 'str', 'ID_RELACION': 'str', 'COD_EMPRESA': 'str',\n",
    "                 'CLAVE_LECTURA': 'str', 'CLAVE_TERRENO_MES_ANT': 'str', 'CONS_BASE_MES_ANT': 'float32', 'COD_OBS_MES_ANT': 'str',\n",
    "                 'CLAVE_TERRENO_MISMO_MES_ANNO_ANT': 'str', 'CONS_BASE_MISMO_MES_ANNO_ANT': 'float32',\n",
    "                 'COD_OBS_MISMO_MES_ANNO_ANT': 'str', 'ANNO': 'str', 'MES': 'str', 'ANALISTA': 'float32', 'TIENE_REFA': 'float32'}\n",
    "        Data = pd.read_csv(archivo[0], delimiter=';', dtype=dtype, low_memory=False)\n",
    "    #Reemplazo de nulos -------------------------------------------------------------------------------------------------------\n",
    "    Data['COD_DIAMETRO']=Data['COD_DIAMETRO'].fillna('13')\n",
    "    Data['CATEGORIA']=Data['CATEGORIA'].fillna('R')\n",
    "    Data['COD_LECTOR']=Data['COD_LECTOR'].fillna('0')\n",
    "    Data['TIE_REMAR']=Data['TIE_REMAR'].fillna('N')\n",
    "    Data['COD_OBS']=Data['COD_OBS'].fillna('ACE')\n",
    "    Data['COD_OBS_MES_ANT']=Data['COD_OBS_MES_ANT'].fillna('ACE')\n",
    "    Data['COD_OBS_MISMO_MES_ANNO_ANT']=Data['COD_OBS_MISMO_MES_ANNO_ANT'].fillna('ACE')\n",
    "    Data['CLAVE_TERRENO_MES_ANT']=Data['CLAVE_TERRENO_MES_ANT'].fillna('N')\n",
    "    Data['CLAVE_TERRENO_MISMO_MES_ANNO_ANT']=Data['CLAVE_TERRENO_MISMO_MES_ANNO_ANT'].fillna('N')\n",
    "    Data['CONSUMO_BASE']=Data['CONSUMO_BASE'].fillna(0)\n",
    "    Data['CONS_BASE_MES_ANT']=Data['CONS_BASE_MES_ANT'].fillna(0)\n",
    "    #pre-procesamiento de algunas columnas ------------------------------------------------------------------------------------\n",
    "    Data.CATEGORIA = Data.CATEGORIA.replace({\"G\": 'R', \"1\": 'R', \"O\": 'F'})\n",
    "    Data.COD_DIAMETRO = Data.COD_DIAMETRO.replace({\"13\": '013'})\n",
    "    Data = Data.drop(Data[Data['CATEGORIA']=='F'].index)\n",
    "    #Filtrado de datos -----------------------------------------------------------------------------------------------------------\n",
    "    if filtrado == True:\n",
    "        #Filtrado casos atipicos 03-2016 LISTO\n",
    "        Data = Data.drop(Data[((Data['MES']=='3') & (Data['ANNO']=='2016') & (Data['COD_EMPRESA']=='1') & (Data['TIENE_REFA']==1)) &\n",
    "                              ((Data['COD_OBS']=='ACE') | (Data['COD_OBS']=='DXS') | (Data['COD_OBS']=='DHS') | (Data['COD_OBS']=='CXS') | (Data['COD_OBS']=='LAM') | (Data['COD_OBS']=='ASM') | (Data['COD_OBS']=='SUS')\n",
    "                               | (Data['COD_OBS']=='BYP')) &\n",
    "                              ((Data['CLAVE_TERRENO']=='1') | (Data['CLAVE_TERRENO']=='L') | (Data['CLAVE_TERRENO']=='N') | (Data['CLAVE_TERRENO']=='R'))\n",
    "                             ].index)\n",
    "        #Filtrado casos atipicos 10-2016 LISTO\n",
    "        Data = Data.drop(Data[((Data['MES']=='10') & (Data['ANNO']=='2016') & (Data['COD_EMPRESA']=='2') & (Data['TIENE_REFA']==1)) &\n",
    "                              ((Data['COD_OBS']=='ACE') | (Data['COD_OBS']=='CVE')) &\n",
    "                              (Data['CLAVE_TERRENO']=='N')].index)\n",
    "        #Filtrado casos atipicos 05-2017 LISTO\n",
    "        Data = Data.drop(Data[((Data['MES']=='5') & (Data['ANNO']=='2017') & (Data['COD_EMPRESA']=='2') & (Data['TIENE_REFA']==1)) &\n",
    "                              ((Data['RECORR1']=='08')|(Data['RECORR1']=='07')|(Data['RECORR1']=='06')|(Data['RECORR1']=='05')|\n",
    "                               (Data['RECORR1']=='04')|(Data['RECORR1']=='03')|(Data['RECORR1']=='02')|(Data['RECORR1']=='01')) &\n",
    "                              ((Data['COD_OBS']=='ACE') | (Data['COD_OBS']=='CC') | (Data['COD_OBS']=='DHS') | (Data['COD_OBS']=='CVE') | (Data['COD_OBS']=='LAM') | (Data['COD_OBS']=='ASM') | (Data['COD_OBS']=='SUS') | (Data['COD_OBS']=='CVA')) &\n",
    "                              ((Data['CLAVE_TERRENO']=='N') | (Data['CLAVE_TERRENO']=='2') | (Data['CLAVE_TERRENO']=='1') | (Data['CLAVE_TERRENO']=='8') | (Data['CLAVE_TERRENO']=='I') | (Data['CLAVE_TERRENO']=='L') | (Data['CLAVE_TERRENO']=='P'))].index)\n",
    "        #Filtrado casos atipicos 06-2017\n",
    "        Data = Data.drop(Data[((Data['MES']=='6') & (Data['ANNO']=='2017') & (Data['COD_EMPRESA']=='2') & (Data['TIENE_REFA']==1)) &\n",
    "                              ((Data['RECORR1']=='09')|(Data['RECORR1']=='10')|(Data['RECORR1']=='11')|(Data['RECORR1']=='12')|\n",
    "                               (Data['RECORR1']=='13')|(Data['RECORR1']=='14')|(Data['RECORR1']=='15')|(Data['RECORR1']=='16')|\n",
    "                               (Data['RECORR1']=='17')|(Data['RECORR1']=='18')|(Data['RECORR1']=='19')|(Data['RECORR1']=='20')) &\n",
    "                              ((Data['COD_OBS']=='ACE') | (Data['COD_OBS']=='CC') | (Data['COD_OBS']=='DHS') | (Data['COD_OBS']=='CVE') | (Data['COD_OBS']=='LAM') | (Data['COD_OBS']=='ASM') | (Data['COD_OBS']=='SUS') | (Data['COD_OBS']=='DPU') | (Data['COD_OBS']=='LDE')) &\n",
    "                              ((Data['CLAVE_TERRENO']=='N') | (Data['CLAVE_TERRENO']=='2') | (Data['CLAVE_TERRENO']=='1') | (Data['CLAVE_TERRENO']=='8') | (Data['CLAVE_TERRENO']=='3') | (Data['CLAVE_TERRENO']=='I') | (Data['CLAVE_TERRENO']=='L') | (Data['CLAVE_TERRENO']=='P') | (Data['CLAVE_TERRENO']=='T'))].index)\n",
    "        #Filtrado casos atipicos 06-2018 LISTO\n",
    "        Data = Data.drop(Data[((Data['MES']=='6') & (Data['ANNO']=='2018') & (Data['COD_EMPRESA']=='1') & (Data['TIENE_REFA']==1)) &\n",
    "                              (Data['COD_OBS']=='CC') &\n",
    "                              ((Data['CLAVE_TERRENO']=='1') | (Data['CLAVE_TERRENO']=='2'))].index)\n",
    "    else:\n",
    "        Data = Data.drop(Data[(Data['MES']=='3') & (Data['ANNO']=='2016')].index)\n",
    "        Data = Data.drop(Data[(Data['MES']=='10') & (Data['ANNO']=='2016')].index)\n",
    "        Data = Data.drop(Data[(Data['MES']=='5') & (Data['ANNO']=='2017')].index)\n",
    "        Data = Data.drop(Data[(Data['MES']=='6') & (Data['ANNO']=='2017')].index)\n",
    "        Data = Data.drop(Data[(Data['MES']=='6') & (Data['ANNO']=='2018')].index)\n",
    "    #Reordenar los indices de dataframe\n",
    "    Data = Data.reset_index(drop=True)\n",
    "    #Agregar columna de responsabilidad del analista --------------------------------------------------------------------------\n",
    "    Data.insert(len(Data.columns)-1, 'ANALISTA', 0)\n",
    "    #codigos de responsabilidad del analista\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'P', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'A', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'B', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'C', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'G', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'M', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'O', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'W', 'ANALISTA'] = 1\n",
    "    Data.loc[Data['CLAVE_LECTURA'] == 'Z', 'ANALISTA'] = 1\n",
    "    \n",
    "    print('Se Cargo base de datos')\n",
    "    return Data\n",
    "\n",
    "def Preprocesamiento_Datos(Data, ohe=None, mini=None, maxi=None, SS=None, SS_num=None, verbose=1):\n",
    "    if (ohe is None) and (mini is None) and (maxi is None) and (SS is None) and (SS_num is None):\n",
    "        codificacion=True\n",
    "    else:\n",
    "        codificacion=False\n",
    "    \n",
    "    #eliminacion de algunas columnas que no sirven\n",
    "    Data = Data.drop('NRO_SUMINISTRO', axis=1)\n",
    "    Data = Data.drop('COD_LECTOR', axis=1)\n",
    "    Data = Data.drop('ANNO', axis=1)\n",
    "    \n",
    "    #guardar columnas que sean float\n",
    "    columnas = Data.columns\n",
    "    col = np.zeros(len(Data.columns))\n",
    "    j = 0\n",
    "    for i in columnas:\n",
    "        if Data[i].dtype == 'f':\n",
    "            col[j] = 1\n",
    "        j = j + 1\n",
    "            \n",
    "    x_train = Data.values\n",
    "    del Data\n",
    "            \n",
    "    x_train = x_train.astype(str)\n",
    "\n",
    "    ######################################################################################################################\n",
    "    #Preparar datos para codificacion\n",
    "    x_train = np.transpose(x_train)\n",
    "            \n",
    "    #Separar datos numericos de categoricos\n",
    "    init = 0\n",
    "    k = 0\n",
    "    for i in range(0, len(col)-1):\n",
    "        if col[i] == 1:\n",
    "            if init == 2:\n",
    "                x_train_num = np.append(x_train_num, [x_train[k]], axis=0)\n",
    "                x_train = np.delete(x_train, k, axis=0)\n",
    "                k = k - 1\n",
    "            if init == 1:\n",
    "                x_train_num = np.append([x_train_num], [x_train[k]], axis=0)\n",
    "                x_train = np.delete(x_train, k, axis=0)\n",
    "                k = k - 1\n",
    "                init = 2\n",
    "            if init == 0:\n",
    "                x_train_num = x_train[k]\n",
    "                x_train = np.delete(x_train, k, axis=0)\n",
    "                k = k - 1\n",
    "                init = 1\n",
    "            k = k + 1\n",
    "        else:\n",
    "            k = k + 1\n",
    "    \n",
    "    #codificacion OneHot para datos categorios ---------------------------------------------------------------------------\n",
    "    x_train = np.transpose(x_train)\n",
    "    \n",
    "    if ohe is None:\n",
    "        ohe = OneHotEncoder(dtype='float16',handle_unknown='ignore')\n",
    "        ohe.fit(x_train)\n",
    "    \n",
    "    x_train = ohe.transform(x_train)\n",
    "    \n",
    "    #StandarScaler\n",
    "    if SS is None:\n",
    "        SS = StandardScaler(with_mean=False)\n",
    "        SS.fit(x_train) \n",
    "    x_train = SS.transform(x_train)\n",
    "    \n",
    "    x_train = np.transpose(x_train)\n",
    "            \n",
    "    #estandarizacion minimax para datos numericos ----------------------------------------------------------------------\n",
    "    x_train_num = x_train_num.astype('float')\n",
    "    \n",
    "    if (mini is None) & (maxi is None):\n",
    "        mini = np.zeros(len(x_train_num))\n",
    "        maxi = np.zeros(len(x_train_num))\n",
    "        for i in range(0, len(x_train_num)):\n",
    "            x_train_num[:,i], mini[i], maxi[i] = norm_minimax(datos_train=x_train_num[:,i], splits=False)\n",
    "    if (mini is not None) & (maxi is not None):\n",
    "        for i in range(len(x_train_num)):\n",
    "            x_train_num[i,:] = norm_minimax(datos_train=x_train_num[i,:], splits=False, mini=mini[i], maxi=maxi[i])\n",
    "    \n",
    "    #StandarScaler  \n",
    "    x_train_num = np.transpose(x_train_num)\n",
    "    if SS_num is None:\n",
    "        SS_num = StandardScaler()\n",
    "        SS_num.fit(x_train_num)\n",
    "    x_train_num = SS_num.fit_transform(x_train_num)\n",
    "    \n",
    "    x_train_num = np.transpose(x_train_num)\n",
    "    \n",
    "    #volver a unir los datos -------------------------------------------------------------------------------------------\n",
    "    x_train = scipy.sparse.vstack((x_train, x_train_num))\n",
    "    del x_train_num\n",
    "    \n",
    "    x_train = np.transpose(x_train)\n",
    "\n",
    "    if verbose == 1:\n",
    "        print('Codificacion lista')\n",
    "        print('Dimensiones datos: ',x_train.shape)\n",
    "    \n",
    "    \n",
    "    if (codificacion==False):\n",
    "        return x_train\n",
    "    \n",
    "    else: \n",
    "        print('Error en argumentos de funcion de pre-procesamiento de datos')\n",
    "        return -1\n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "#metricas ha ocupar\n",
    "metricas = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "print('librerias y funciones listas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se Cargo base de datos\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:10.674681\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "#Carga datos\n",
    "archivo = ['../data-analista.csv']\n",
    "Data = Carga_Datos(archivo=archivo, limpia=False)\n",
    "\n",
    "# agregar columna de diferencia de consumos por temporada\n",
    "data_consumos = joblib.load('../data_consumos.pkl')\n",
    "Data = Data.merge(data_consumos, on='NRO_SUMINISTRO', how='left')\n",
    "Data.insert(len(Data.columns)-2, 'DIFF_CONSUMOS', 0)\n",
    "Data['DIFF_CONSUMOS'] = Data['diff_consumos']\n",
    "Data = Data.drop('diff_consumos', axis=1)\n",
    "Data = Data.dropna()\n",
    "\n",
    "# eliminacion de algunas columnas que no sirven\n",
    "Data = Data.drop('ANALISTA', axis=1)\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se guardo el archivo de salida\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:04.458329\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# cargar codificaciones\n",
    "ohe = joblib.load('../ohe.pkl')\n",
    "mini = joblib.load('../mini.pkl')\n",
    "maxi = joblib.load('../maxi.pkl')\n",
    "SS = joblib.load('../SS.pkl')\n",
    "SS_num = joblib.load('../SS_num.pkl')\n",
    "le = joblib.load('../le.pkl')\n",
    "\n",
    "#definicion de temporadas\n",
    "temporadas = ['1', '2', 'neutra']\n",
    "\n",
    "predprob_y = np.array([])\n",
    "y_test = np.array([])\n",
    "for temporada in temporadas:\n",
    "    if temporada == '1':\n",
    "        Data_test_temp = Data.drop(Data[Data['DIFF_CONSUMOS'] <= 0.5].index)\n",
    "    if temporada == '2':\n",
    "        Data_test_temp = Data.drop(Data[Data['DIFF_CONSUMOS'] >= -0.5].index)\n",
    "    if temporada == 'neutra':\n",
    "        Data_test_temp = Data.drop(Data[(Data['DIFF_CONSUMOS'] > 0.5) | (Data['DIFF_CONSUMOS'] < -0.5)].index)\n",
    "            \n",
    "    Data_test_temp = Data_test_temp.drop('DIFF_CONSUMOS', axis=1)\n",
    "    \n",
    "    if len(Data_test_temp) != 0:\n",
    "        # guardar numero de suministro de clientes\n",
    "        if temporada == '1':\n",
    "            registros = Data_test_temp['NRO_SUMINISTRO']\n",
    "        else:\n",
    "            registros = pd.concat([registros, Data_test_temp['NRO_SUMINISTRO']], axis=0)\n",
    "        # codificacion\n",
    "        x_test = Preprocesamiento_Datos(Data_test_temp,\n",
    "                                        ohe=ohe, mini=mini, maxi=maxi, SS=SS, SS_num=SS_num, verbose=0)\n",
    "        # cargando el modelo desde el disco\n",
    "        filename = 'archivos_carga_modelos/modelo_NN_DA_temporada_' + str(temporada) + '.json'\n",
    "        filename_pesos = 'archivos_carga_modelos/pesos_NN_DA_temporada_' + str(temporada) + '.h5'\n",
    "        json_file = open(filename, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model_NN = model_from_json(loaded_model_json)\n",
    "        # cargar los pesos del nuevo modelo\n",
    "        model_NN.load_weights(filename_pesos)\n",
    "        # compilar el modelo\n",
    "        model_NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=metricas)\n",
    "        # hacer prediccion\n",
    "        predprob_y = np.append(predprob_y, model_NN.predict_proba(x_test.tocsr()))\n",
    "        #y_test = np.append(y_test, y_test_temp)\n",
    "\n",
    "# resetiar indices de numero de registro\n",
    "registros = registros.reset_index(drop=True)\n",
    "        \n",
    "# pasar las probabilidades de predicion a dataframe\n",
    "predicciones = pd.DataFrame(predprob_y, columns = ['PROB_REFACTURA'])\n",
    "\n",
    "#unir ambos dataframe\n",
    "salida = pd.concat([registros, predicciones], axis=1)\n",
    "\n",
    "# crear archivo de salida\n",
    "nombre_salida = 'prob_refactura_NN_DA_temporada.csv' \n",
    "salida.to_csv(nombre_salida, sep=';', index=False)\n",
    "print('Se guardo el archivo de salida')\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
